<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>PL-NeRF</title>

    <meta name="description" content="NeRF Revisited: Fixing Quadrature Instability in Volume Rendering">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://bakedsdf.github.io/img/density.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://pl-nerf.github.io"/>
    <meta property="og:title" content="PL-NeRF" />
    <meta property="og:description" content="Project page for NeRF Revisited: Fixing Quadrature Instability in Volume Rendering." />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="PL-NeRF" />
    <meta name="twitter:description" content="Project page for NeRF Revisited: Fixing Quadrature Instability in Volume Rendering." />
    <meta name="twitter:image" content="https://bakedsdf.github.io/img/density.png"/>


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üçû</text></svg>">
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
	
<!--
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>
-->
	
	<link rel="stylesheet" href="css/dics.min.css">
    <script src="scripts/dics.min.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', domReady);
        function domReady() {
            for (const e of document.querySelectorAll(".b-dics")) {
                new Dics({
                    container: e,
                    textPosition: "top"
                });
            }
        }
    </script>
	
    <!--ADDED FOR JUXTAPOSE JS-->
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=0">
    <link rel="stylesheet" href="https://cdn.knightlab.com/libs/juxtapose/latest/css/juxtapose.css">
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>NeRF Revisited: Fixing Quadrature Instability in Volume Rendering</b> <br>
                <small>
                    NeurIPS 2023
                </small>
            </h2>
        </div>
		
		<div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://mikacuy.github.io/">
                          Mikaela Angelina Uy
                        </a>
                        <br>Stanford University<br><span>&#8203;</span>
                    </li>
					<li>
                        <a href="https://georgenakayama.github.io">
                          George Kiyohiro Nakayama
                        </a>
                        <br>Stanford University<br><span>&#8203;</span>
                    </li>
	
						<li>
                        <a href="https://www.guandaoyang.com/">
                          Guandao Yang
                        </a>
                        <br>Cornell University<br>Stanford University
                    </li>
                    <li>
                        Rahul Krishna Thomas
                        <br>Stanford University<br><span>&#8203;</span>
                    </li><br><br>
					<li>
                        <a href="https://geometry.stanford.edu/member/guibas/">
                          Leonidas Guibas
                        </a>
                        <br>Stanford University<br><span>&#8203;</span>
                    </li>

                    <li>
                        <a href="https://www.sfu.ca/~keli/">
                          Ke Li
                        </a>
                        <br>Simon Fraser University<br>Google
                    </li> 				

                </ul>
            </div>
        </div>
		

        <div class="row">
                <div class="col-md-8 col-md-offset-2 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
							<a href="https://arxiv.org/abs/2310.20685" target="_blank">
                            <image src="img/plnerf_paper_image.png" height="120px"></image>
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://youtu.be/0QGlsQqbDkM" target="_blank">
                            <image src="img/youtube_icon.png" height="120px"></image>
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/mikacuy/PL-NeRF">
                            <image src="img/github_icon.png" height="120px"></image>
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <!-- <image src="img/rays.jpg" class="img-responsive" alt="overview"><br> -->
                <p class="text-justify">
  Neural radiance fields (NeRF) rely on volume rendering to synthesize novel views. Volume rendering requires evaluating an integral along each ray, which is numerically approximated with a finite sum that corresponds to the exact integral along the ray under <em>piecewise constant volume density</em>. As a consequence, the rendered result is unstable w.r.t. the choice of samples along the ray, a phenomenon that we dub <b>quadrature instability</b>. We propose a mathematically principled solution by reformulating the sample-based rendering equation so that it corresponds to the exact integral under <b>piecewise linear volume density</b>. This simultaneously resolves multiple issues: conflicts between samples along different rays, imprecise hierarchical sampling, and non-differentiability of quantiles of ray termination distances w.r.t. model parameters. We demonstrate several benefits over the classical sample-based rendering equation, such as sharper textures, better geometric reconstruction, and stronger depth supervision. Our proposed formulation can be also be used as a drop-in replacement to the volume rendering equation for existing methods like NeRFs.
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <!-- Coming Soon! <br> -->
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/0QGlsQqbDkM" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <br>What We <s>Are</s> Were All Used To</br>
                </h3>
                <p>
                    One of the key underpinnings of the recent advances of coordinate-based representations, e.g. NeRFs, is volume rendering. Volume rendering enables end-to-end differentiable rendering, and hence has made learning of 3D geometry and appearance from only 2D images possible. In practice, the volume rendering integral is evaluated with <b>quadrature</b> resulting in the expressions shown below.    
                </p>
                <br>
                <image src="img/constant_pdf.png" style="width:55%;" class="img-responsive center-block" alt="overview"></image>
                <br>
                <image src="img/constant_transmittance.png" style="width:45%;" class="img-responsive center-block" alt="overview"></image>
                <br>                
                <p class="text-justify">
                    These expressions are what we've gotten used to, which is the exact integral under the <b>piecewise constant</b> assumption to opacity and color. However, this seemingly simple, innocuous assumption can result in the rendered image being sensitive to the choice of samples along the ray. While this does not necessarily cause a practical issue in classical rendering pipelines, it has surprising consequences when used in neural rendering.  
                </p>     
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    PL-NeRF
                </h3>
                <br>
                <image src="img/grazing_angle_v4.png" style="width:100%;" class="img-responsive center-block" alt="overview"></image>
                <br>
                <p class="text-justify">
                    We show that the piecewise constant assumption to opacity causes sensitivity to the choice of samples at both rendering and sampling time, which we dub as <b>quadrature instability</b>. This can lead to a number of issues: i) conflicting ray supervision leading to fuzzy surfaces, ii) imprecise samples and iii) lack of supervision on the CDF from samples.
                </p>            
<!--                 <p>
                In this paper, we revisit the quadrature used to approximate volume rendering in NeRF and devise a different quadrature formula based on a different approximation to the opacity. We first show that interestingly a closed-form expression can be derived under any piecewise polynomial approximation for opacity. When the polynomial degree is 0, it reduces to the piecewise constant opacity as in existing literature, and when the degree is 2 or more, we show that it would lead to poor numerical conditioning.
                </p>          -->          
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Volume Rendering with Piecewise Linear Opacity
                </h3>
                <br>
                <image src="img/density.png" style="width:70%;" class="img-responsive center-block" alt="overview"></image>
                <br>  
                <p class="text-justify">
                    
                    We revisit the quadrature used to approximate volume rendering in NeRF and devise a different quadrature formula based on a different approximation to the opacity. We derive the rendering equation under <b>piecewise linear</b> opacity and show that it both resolves quadrature instability and has good numerical conditioning.
                </p>                
                <br>
                <image src="img/linear_pdf.png" style="width:70%;" class="img-responsive center-block" alt="overview"></image>
                <image src="img/linear_transmittance.png" style="width:60%;" class="img-responsive center-block" alt="overview"></image>
                <br>                
                <p class="text-justify">
                    We show that it has a <b>simple and intuitive form</b> and results in a <b>new quadrature</b> method for volume rendering, which can serve as a <b>drop-in replacement</b> for existing methods like NeRFs. We demonstrate that this reduces artifacts, improves rendering quality and results in better geometric reconstruction.
                </p>
                <br>
                <image src="img/precise_importance_sampling.png" style="width:70%;" class="img-responsive center-block" alt="overview"></image>
                <br>
                <p>
                    We also devise a new way to sample <b>directly</b> from the distribution of samples along each ray induced by NeRF without going through the surrogate, which opens the way to a more refined importance sampling approach and a more effective method to supervise samples using depth.
                </p>                
            </div>
        </div>

		
		<div class="row comp-margin">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    NeRF Results 
                </h3>
                <p> <br />We highlight the comparative advantage of <b>PL-NeRF (linear)</b> over <b>Vanilla NeRF (constant)</b> through a zoomed-in render of the side of a microphone.
                    In the constant model, conflicts in ray supervision between perpendicular and grazing angle rays result in <b>fuzzier surfaces</b> . 
                    PL-NeRF alleviates this issue and yields a crisper textures on the microphone.
                </p>
                <div class="b-dics" style="width: 100%">
                    <img src="img/nerf/const_mic.jpeg" alt="Vanilla NeRF (Constant)" />
                    <img src="img/nerf/lin_mic.jpeg" alt="PL-NeRF (Linear)" />
                </div>
                <p><br> We also show qualitative video results from Blender scenes, which illustrate the improvements of the linear model over the constant model at various rendered camera views.</p>
                <div class="b-dics" style="width: 100%" class="img-responsive center-block">
                    <img src="img/gifs/chair_constant_multidist_slow (1).gif" alt="Vanilla NeRF (Constant)" />
                    <img src="img/gifs/chair_linear_multidist_slow (1).gif" alt="PL-NeRF (Linear)" />
                </div>
                <p><br><b>Chair: </b>When the camera is positioned close to the chair, the slider shows sharper texturing on the chair patterns for the linear model than the constant model.</p>
                <div class="b-dics" style="width: 100%" class="img-responsive center-block">
                    <img src="img/gifs/mic_constant_hemis_slow (1).gif" alt="Vanilla NeRF (Constant)" />
                    <img src="img/gifs/mic_linear_hemis_slow (1).gif" alt="PL-NeRF (Linear)" />
                </div>
                <p><br /><b>Microphone: </b>At a bird's eye view, the linear model captures the internal structure of the microphone, which is lost in constant volume rendering.</p>
            </div>
        </div>

        <div class="row comp-margin">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Multidistance Results
                </h3>
                <p><br> We further consider the challenging setting of training with cameras at <b>different distances</b> to the object. 
                    This results in <b>different sets of ray samples</b> leading to quadrature instability in the constant model.
                    PL-NeRF results in better renderings on <b>multi-distance</b> views: at all distances, the rendered outputs for PL-NeRF have sharper texture than Vanilla NeRF.
                    Furthermore, in Vanilla NeRF the level of noise (gold specs) and blurriness vary at different camera distances, whereas our PL-NeRF renders <b>crisper and more consistent</b> outputs over all distances.
                </p>
                <p>
                    <br />
                    <b>1x Camera-to-Scene Distance:</b>
                    <br />
                </p>
                <div class="b-dics" style="width: 100%">
                    <img src="img/multidist_new/const_1x.jpeg" alt="Vanilla NeRF (Constant)" />
                    <img src="img/multidist_new/lin_1x.jpeg" alt="PL-NeRF (Linear)" />
                </div>
                <p>
                    <br />
                    <b>0.5x Camera-to-Scene Distance:</b>
                    <br />
                </p>
                <div class="b-dics" style="width: 100%">
                    <img src="img/multidist_new/const_0.5x.jpeg" alt="Vanilla NeRF (Constant)" />
                    <img src="img/multidist_new/lin_0.5x.jpeg" alt="PL-NeRF (Linear)" />
                </div>
                <p>
                    <br />
                    <b>0.25x Camera-to-Scene Distance:</b>
                    <br />
                </p>
                <div class="b-dics" style="width: 100%">
                    <img src="img/multidist_new/const_0.25x.jpeg" alt="Vanilla NeRF (Constant)" />
                    <img src="img/multidist_new/lin_0.25x.jpeg" alt="PL-NeRF (Linear)" />
                </div>
                <br>
            </div>
        </div>

        <div class="row comp-margin">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Mip-NeRF Results
                </h3>
                <p>We also integrate our piecewise linear opacity formulation into <a href="https://jonbarron.info/mipnerf/">Mip-NeRF</a>, which we call <b>PL-MipNeRF</b>. 
                    We see that under difficult scenarios such as when ray conflicts arise in the fine texture details of the Chair and in the presence of grazing angle views in the Mic, our PL-MipNeRF shows significant improvement over the baseline. 
                    This shows that our approach can be used as a <b>drop-in replacement</b> to existing <b>NeRF variants</b>.
                <br>
            </p>
                <img src="img/mipnerf/mipnerf_chair.jpeg" style="width: 100%">
                <img src="img/mipnerf/mipnerf_mic.jpeg" style="width: 100%">
                <br>
            </div>
        </div>
        
        <div class="row comp-margin">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Geometric Extraction
                </h3>
                <p>
                    PL-NeRF also improves <b>geometric reconstruction</b> on coordinate-based networks. 
                    We can extract the geometry of a scene from the learned density field in the trained models of PL-NeRF and Vanilla NeRF.
                    Below we show qualitative improvements in the reconstruction from the piecewise linear model, compared to the original piecewise constant formulation. 
                </p>
                <p>
                    <b>Microphone: </b>PL-NeRF is able to better recover the structure inside the Mic, which is lost in the constant model.
                    <br />
                </p>
                <div class="b-dics" style="width: 100%">
                    <img src="img/geometry/const_scope.jpeg" alt="Constant" />
                    <img src="img/geometry/lin_scope.jpeg" alt="Linear" />
                </div>
                <p> <br>
                    <b>Lego: </b>The holes on the body and wheels of the Lego scene become visually clearer in PL-NeRF.
                    <br />
                </p>
                <div class="b-dics" style="width: 100%">
                    <img src="img/geometry_lego/const_lego.jpeg" alt="Constant" />
                    <img src="img/geometry_lego/lin_lego.jpeg" alt="Linear" />
                </div>
                <p> <br>
                    <b>Drums: </b>Interestingly, the interior of a hollow drum is reconstructed as hollow in PL-NeRF.
                </p>
                <div class="b-dics" style="width: 100%">
                    <img src="img/geometry/const_drums.jpeg" alt="Constant" />
                    <img src="img/geometry/lin_drums.jpeg" alt="Linear" />
                </div>
                <p> <br>
                    Even if the ground truth mesh is solid at the drum surface, PL-NeRF visually matches the image of the transparent drum (i.e. the only
                    signal used in supervised learning) better than NeRF, as shown below.
                    <br />
                </p>
                <img src="img/geometry/drum_img.jpeg" style="width:85%;" class="img-responsive center-block" alt="GT" />
            </div>
        </div>
        

		<br>
			<div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{uy-plnerf-neurips23,
      title = {NeRF Revisited: Fixing Quadrature Instability in Volume Rendering},
      author = {Mikaela Angelina Uy and George Kiyohiro Nakayama and Guandao Yang and Rahul Krishna Thomas and Leonidas Guibas and Ke Li},
      booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
      year = {2023}
}
</textarea>
                </div>
            </div>
        </div>	    

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                This work is supported by a Apple Scholars in AI/ML PhD Fellowship, a Snap Research Fellowship, a Vannevar Bush Faculty Fellowship, ARL grant W911NF-21-2-0104, a gift from the Adobe corporation, the Natural Sciences and Engineering Research Council of Canada (NSERC), the BC DRI Group and the Digital Research Alliance of Canada. 
                </p>
                <br>
                The website template was borrowed from <a href="https://bakedsdf.github.io">BakedSDF</a>.
            </div>
        </div>

    </div>
</body>
</html>
